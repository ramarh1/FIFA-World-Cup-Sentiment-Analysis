{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramarh1/FIFA-World-Cup-Sentiment-Analysis/blob/main/tweet_final_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykEjS8wvAWyW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8KZquSYoLBG"
      },
      "source": [
        "**References**\n",
        "\n",
        "https://medium.com/@piocalderon/vader-sentiment-analysis-explained-f1c4f9101cd9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXKYkyeKAWyb"
      },
      "source": [
        "# **Read Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "mIbzf0lPAWyb",
        "outputId": "3711d5e0-0fa7-4a9f-a663-5bef6da7f88c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f96a08f911aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./fifa_world_cup_2022_tweets.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './fifa_world_cup_2022_tweets.csv'"
          ]
        }
      ],
      "source": [
        "tweets = pd.read_csv(\"./fifa_world_cup_2022_tweets.csv\", encoding='utf-8')\n",
        "tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjsbeM3qGJgy"
      },
      "outputs": [],
      "source": [
        "#Check number columns and their name\n",
        "tweets.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7cM_MZDAWyc"
      },
      "source": [
        "## **1. Visualization of the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wW5Nul8SAWyc"
      },
      "outputs": [],
      "source": [
        "# Breakdown of tweets' sentiments\n",
        "\n",
        "# setting figure size\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "#make the bar chart\n",
        "ax = pd.value_counts(tweets['Sentiment']).plot.bar(\n",
        "    color=['purple', 'yellow', 'black'])\n",
        "\n",
        "#call function to add value labels\n",
        "for p in ax.patches:\n",
        "   ax.annotate('{:.1f}'.format(p.get_height()),\n",
        "               (p.get_x()+0.25, p.get_height()+0.01))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf4qCU2LoLBJ"
      },
      "source": [
        "It looks like most of the tweets in the FIFA world cup is deemed positive. However, the predetermined sentiment tag does not seem to do well in deciding the tweets' sentiments if the emotions are not at extreme ends, resulting in a lot of neutral sentiment tags as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVKRgzW2AWyc"
      },
      "outputs": [],
      "source": [
        "# plot for sources of the tweets\n",
        "sources = tweets['Source of Tweet'].value_counts()[:5]\n",
        "labels = ['Twitter for iPhone', 'Twitter for Android',\n",
        "          'Twitter Web App', 'TweetDeck', 'Twitter for iPa']\n",
        "plt.pie(sources, labels=labels, autopct='%1.2f%%')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY-ZnHXOoLBJ"
      },
      "source": [
        "Most of the tweets seem to come from mobile platform, which makes sense, because Twitter is mobile-oriented social media platform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd-2L0s5AWyc"
      },
      "source": [
        "## **2. Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVDrz1SIAWyd"
      },
      "source": [
        "### 2.1 Extract urls, hashtags, usernames and remove them from original text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7LmxGtFAWyd"
      },
      "outputs": [],
      "source": [
        "#extract all the urls in the tweets into an object\n",
        "url = tweets.Tweet.str.extract(r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})').head()\n",
        "url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJsDsVK8AWyd"
      },
      "outputs": [],
      "source": [
        "# function to print all the hashtags in a text\n",
        "def extract_hashtags(text):\n",
        "    # splitting the text into words\n",
        "    hashtags = object()\n",
        "    for word in text.split():\n",
        "        if word[0] == \"#\":\n",
        "            hashtags = word[1:]\n",
        "    return(hashtags)\n",
        "\n",
        "\n",
        "#create an object to store all the hashtags in tweets\n",
        "tweets['hashtags'] = tweets['Tweet'].apply(lambda x: extract_hashtags(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nw5brquQGJgz"
      },
      "outputs": [],
      "source": [
        "# Breakdown of the hashtags in the tweets\n",
        "# top 10 most mentioned hashtags\n",
        "ax = tweets['hashtags'].value_counts()[:10].plot(kind='barh', figsize=(5, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aezri5Z1oLBL"
      },
      "source": [
        "The host country Qatar is a hot topic during the FIFA Word Cup period, with it being mentioned quite often.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46A1zVSvAWyd"
      },
      "outputs": [],
      "source": [
        "# Tweets have some newline - need to replace \n",
        "def whitespace_sub(text):\n",
        "    tabs= re.findall( '\\n', text)\n",
        "    for tab in tabs:\n",
        "        text = text.replace(tab[0], ' ')\n",
        "    return text\n",
        "\n",
        "tweets['Tweet']= tweets['Tweet'].apply(whitespace_sub)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNUr8KQSAWyd"
      },
      "outputs": [],
      "source": [
        "# username extraction from tweets\n",
        "def extract_usernames(text):\n",
        "    usernames = ()\n",
        "    for word in text.lower().split():\n",
        "        if word.startswith('@') and len(word) > 1:\n",
        "            usernames = word[1:]\n",
        "    return usernames\n",
        "\n",
        "\n",
        "tweets['usernames'] = tweets['Tweet'].apply(lambda x: extract_usernames(x))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvhYKqplGJgz"
      },
      "outputs": [],
      "source": [
        "tweets1 = tweets[tweets['usernames'].astype(bool)]\n",
        "usernames = tweets1['usernames'].value_counts()[:10]\n",
        "usernames.plot(kind='barh')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl0XCYByoLBL"
      },
      "source": [
        "BTS, the opening singer for the WordCup, and Elon Musk, the new Twitter's CEO, are mentioned the most during the event period"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcOOqpnDAWyd"
      },
      "outputs": [],
      "source": [
        "tweets.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l___xf5mAWyf"
      },
      "source": [
        "### 2.2 Remove stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiHspdroAWyf"
      },
      "outputs": [],
      "source": [
        "#download the stopword library\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "#save stop words as a set\n",
        "stop_words = set(stopwords.words(\"english\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Poc3JEzPAWyf"
      },
      "outputs": [],
      "source": [
        "#custom stop words - to minimize repeated words\n",
        "new_stop_words = ['FIFA','WordCup2022','World Cup', 'worldcup', 'qatar', 'qatar2022','world','cup','worldcup2022','qatarworldcup2022', 'qatar2022', '2022',\n",
        "                  'fifaworldcup','qatarworldcup2022','football', 'fifa', 'qatarworldcup']\n",
        "all_stop_words = stop_words.union(new_stop_words)\n",
        "\n",
        "# convert the stop words into a list\n",
        "all_stop_words_list = list(all_stop_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Kr5udwRAWyf"
      },
      "outputs": [],
      "source": [
        "#stemming\n",
        "snow = nltk.stem.SnowballStemmer('english')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZswMaSs-AWyf"
      },
      "outputs": [],
      "source": [
        "# Combine all above to one function to fix texts\n",
        "def fix_Text(text):\n",
        "\tletters = re.sub(\"[^a-zA-Z]https?:\\/\\/\\S*\", \" \", str(text)) #remove all non-letters and urls\n",
        "\tletters_1 = re.sub(\"#[A-Za-z0-9_]+\", \"\",str(letters)) #remove all hashtags\n",
        "\tletters_2 = re.sub(\"@[A-Za-z0-9_]+\", \"\",str(letters_1)) #remove all mentions\n",
        "\tletters_3 = re.sub(r'[^\\x00-\\x7F]+', ' ', str(letters_2))\n",
        "\n",
        "\twords = letters_3.lower().split() #make all letters lowercase\n",
        "\tmeaningful = [snow.stem(word)\n",
        "               for word in words if word not in all_stop_words] #convert to stemmed words\n",
        "\treturn (\" \".join(meaningful)) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlA8GaR-AWyf"
      },
      "outputs": [],
      "source": [
        "#apply the function on all the tweets\n",
        "clean_tweets = tweets.Tweet.apply(fix_Text)\n",
        "clean_tweets.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROEQYKiOAWyf"
      },
      "outputs": [],
      "source": [
        "#check for empty documents\n",
        "clean_tweets.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWsqDSHOf6JV"
      },
      "outputs": [],
      "source": [
        "# fix the format on sentiment tag\n",
        "def fix_sentiment(df):\n",
        "  df = df.replace(['NEU','NEUTRAL'], 'neutral')\n",
        "  df = df.replace('POS', 'positive')\n",
        "  df = df.replace('NEG', 'negative')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unIAQrMJoLBM"
      },
      "source": [
        "### 2.3 Create Word Clouds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dV7ooytooLBM"
      },
      "outputs": [],
      "source": [
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucFwYedyoLBM"
      },
      "outputs": [],
      "source": [
        "#need to convert from object dtypes to string\n",
        "tweets['hashtags'] = tweets['hashtags'].astype(str)\n",
        "# convert the list of hashtags into one global document for the wordcloud\n",
        "hashtags = \" \".join(tweets['hashtags'].tolist())\n",
        "\n",
        "#create the wordcloud of all hashtags\n",
        "hashtag_wordcloud = WordCloud(width=1600, height=800,\n",
        "                              background_color='white').generate(hashtags)\n",
        "\n",
        "# Display the wordcloud\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.imshow(hashtag_wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOzkewytoLBM"
      },
      "outputs": [],
      "source": [
        "# word cloud for all the tweets\n",
        "text_wc = \" \".join(tweets['Tweet'].to_list())\n",
        "wordcloud_tweets = WordCloud(background_color=\"white\").generate(text_wc)\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(wordcloud_tweets, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4NqjKQWoLBM"
      },
      "source": [
        "### 2.4 Create TFIDF and Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLgaHCHNoLBM"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet_tokenizer = TweetTokenizer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qB-MnJ7uoLBM"
      },
      "outputs": [],
      "source": [
        "# Combine all above to one function to fix texts\n",
        "def fix_Text(text):\n",
        "\tletters = re.sub(\"[^a-zA-Z]https?:\\/\\/\\S*\", \" \", str(text)\n",
        "\t                 )  # remove all non-letters and urls\n",
        "\tletters_1 = re.sub(\"#[A-Za-z0-9_]+\", \"\", str(letters))  # remove all hashtags\n",
        "\tletters_2 = re.sub(\"@[A-Za-z0-9_]+\", \"\", str(letters_1)\n",
        "\t                   )  # remove all mentions\n",
        "\tletters_3 = re.sub(r'[^\\x00-\\x7F]+', ' ', str(letters_2))\n",
        "\n",
        "\twords = letters_3.lower().split()  # make all letters lowercase\n",
        "\tmeaningful = [snow.stem(word)\n",
        "               for word in words if word not in all_stop_words]  # convert to stemmed words\n",
        "\treturn (\" \".join(meaningful))\n",
        "\n",
        "\n",
        "#apply function\n",
        "tweets['tweet_lem'] = tweets['Tweet'].apply(lambda x: fix_Text(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mNlmgM0oLBM"
      },
      "outputs": [],
      "source": [
        "def fix_Text2(text):\n",
        "\tletters = re.sub(\"[^a-zA-Z]https?:\\/\\/\\S*\", \" \", str(text)\n",
        "\t                 )  # remove all non-letters and urls\n",
        "\tletters_1 = re.sub(\"#[A-Za-z0-9_]+\", \"\", str(letters))  # remove all hashtags\n",
        "\tletters_2 = re.sub(\"@[A-Za-z0-9_]+\", \"\", str(letters_1)\n",
        "\t                   )  # remove all mentions\n",
        "\tletters_3 = re.sub(r'[^\\x00-\\x7F]+', ' ', str(letters_2))\n",
        "\n",
        "\twords = letters_3.lower().split()  # make all letters lowercase\n",
        "\t# meaningful = [snow.stem(word)\n",
        "  #              for word in words if word not in all_stop_words] #convert to stemmed words\n",
        "\treturn (\" \".join(words))\n",
        "\n",
        "\n",
        "#apply function\n",
        "tweets['tweet_updated'] = tweets['Tweet'].apply(lambda x: fix_Text2(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OR5TRBxLoLBM"
      },
      "outputs": [],
      "source": [
        "# word cloud for all the tweets\n",
        "text_wc = \" \".join(tweets['tweet_updated'].to_list())\n",
        "wordcloud_tweets = WordCloud(background_color=\"white\").generate(text_wc)\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(wordcloud_tweets, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGgZnExmoLBM"
      },
      "outputs": [],
      "source": [
        "cv = CountVectorizer(stop_words=all_stop_words_list, ngram_range=(\n",
        "    1, 1), tokenizer=tweet_tokenizer.tokenize)\n",
        "text_count = cv.fit_transform(tweets['tweet_lem'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSCw5L5EoLBN"
      },
      "outputs": [],
      "source": [
        "text_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2fGOO6nGJg0"
      },
      "source": [
        "##  **3. Sentiment Analysis Using Vader**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PvTfYMLoLBN"
      },
      "source": [
        "We want to create our own sentiment labeling using a different module to see if the sentiment tagging aligns between two methods and potentially reduce the amount of neutral tags because neutraulity does not tell us much."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NF3hAM9lGJg0"
      },
      "outputs": [],
      "source": [
        "!pip3 install vaderSentiment\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "sent = SentimentIntensityAnalyzer()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Go through tweets and save polarity score\n",
        "tweets['scores'] = tweets['Tweet'].apply(lambda Tweet: sent.polarity_scores(str(Tweet)))\n",
        "\n",
        "#Extract compound score from polarity score\n",
        "tweets['compound']=tweets['scores'].apply(lambda score_dict:score_dict['compound'])"
      ],
      "metadata": {
        "id": "afwopTNC35IM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EENvvIJCGJg1"
      },
      "outputs": [],
      "source": [
        "#Creates the Vader Sentiment Analysis Column using polarity score values\n",
        "tweets['vader_analysis']=''\n",
        "tweets.loc[tweets.compound>0,'vader_analysis']='positive'\n",
        "tweets.loc[tweets.compound==0,'vader_analysis']='neutral'\n",
        "tweets.loc[tweets.compound<0,'vader_analysis']='negative'\n",
        "tweets.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM2MpN6DoLBN"
      },
      "outputs": [],
      "source": [
        "tweets['Sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgW2pn6VoLBN"
      },
      "outputs": [],
      "source": [
        "tweets['vader_analysis'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODTLOeX9oLBN"
      },
      "source": [
        "The Vader module reduces the neutral tags but it also decreases the count of negative Tweets in the dataset. From our research, Vader lexicon is trained with consideration of slangs and colloquialisms, so we believe it would be more suitable in evaluatinng Tweets' sentiment. In the Twitter world, users often use slangs, emojis, punctuations, etc to express their ideas, so taking in consideration these nuances would be more useful for us to understand the tweets' sentiments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEaJoga5oLBN"
      },
      "source": [
        "## **4. Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDiAzlhpoLBN"
      },
      "source": [
        "### 4.1 Logistic Regression -- using vader analysis as the known sentiment label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdrCNuwPoLBN"
      },
      "outputs": [],
      "source": [
        "# split dataset to train and test\n",
        "#By default, Sklearn train_test_split will make random partitions for the two subsets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    tweets['Tweet'], tweets['vader_analysis'], test_size=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJC9nHNSoLBN"
      },
      "outputs": [],
      "source": [
        "#create TFIDF on the split sets\n",
        "tf = TfidfVectorizer()\n",
        "tf.fit(X_train)\n",
        "x_train = tf.transform(X_train)\n",
        "x_test = tf.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MaQha7koLBN"
      },
      "outputs": [],
      "source": [
        "#fit model on train test\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#set seed\n",
        "random_seed = 42\n",
        "\n",
        "lr = LogisticRegression(random_state=random_seed)\n",
        "lr.fit(x_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyCvoX2voLBN"
      },
      "outputs": [],
      "source": [
        "lr_predict = lr.predict(x_test)\n",
        "lr_predict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zdF5yhyoLBN"
      },
      "outputs": [],
      "source": [
        "# evaluate accurary\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy = {:.2f}%\".format(accuracy_score(y_test, lr_predict)*100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ql54Rsn_oLBN"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, lr_predict))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Imm0aWZkoLBN"
      },
      "outputs": [],
      "source": [
        "#evaluate confusion matrix\n",
        "cm2 = metrics.confusion_matrix(y_test,lr_predict)\n",
        "disp = metrics.ConfusionMatrixDisplay(\n",
        "    confusion_matrix=cm2, display_labels=lr.classes_)\n",
        "disp.plot()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-n9BEcnzsqN"
      },
      "source": [
        "### 4.2 Naive Bayes Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnAzyzOjGJg1"
      },
      "outputs": [],
      "source": [
        "#By default, Sklearn train_test_split will make random partitions for the two subsets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    text_count, tweets['Sentiment'], test_size=0.2, random_state=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKSCzhVWGJg1"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "#set seed\n",
        "np.random.seed(42)\n",
        "MNB = MultinomialNB()\n",
        "MNB.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QZNkYDGGJg1"
      },
      "outputs": [],
      "source": [
        "nb_predict = MNB.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XA9Tze4tGJg1"
      },
      "outputs": [],
      "source": [
        "accuracy_score = metrics.balanced_accuracy_score(y_test,nb_predict)\n",
        "print(\"NB Accuracy = {:.2f}%\".format(accuracy_score*100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "072ng9K8z1bz"
      },
      "outputs": [],
      "source": [
        "print(f\"Classification Report:{metrics.classification_report(y_test,nb_predict)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_8v76OIz7Wd"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test,nb_predict)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=MNB.classes_)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zePkxQVLGJg1"
      },
      "source": [
        "### 4.3 Random Forest Classifier (Ensemble Learning Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3r0lsKmvQAC"
      },
      "outputs": [],
      "source": [
        "#By default, Sklearn train_test_split will make random partitions for the two subsets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    tweets['tweet_updated'], tweets['Sentiment'], test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EO1UxFz4ttVJ"
      },
      "outputs": [],
      "source": [
        "#convert the tweets to tfidf\n",
        "tf = TfidfVectorizer()\n",
        "tf.fit(X_train)\n",
        "x_train2 = tf.transform(X_train) #matrix\n",
        "x_test2 = tf.transform(X_test) #matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yySdp80iGJg1"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf = RandomForestClassifier(random_state=random_seed)\n",
        "clf = clf.fit(x_train2,y_train)\n",
        "rf_predict = clf.predict(x_test2)\n",
        "accuracy_score2 = metrics.balanced_accuracy_score(y_test,rf_predict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mH38bvnqq1Ru"
      },
      "outputs": [],
      "source": [
        "print(\"RF Accuracy = {:.2f}%\".format(accuracy_score2*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPsg6jnIxSI_"
      },
      "outputs": [],
      "source": [
        "print(f\"Classification Report:{metrics.classification_report(y_test,rf_predict)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwV8WGEVx98j"
      },
      "outputs": [],
      "source": [
        "#evaluate confusion matrix\n",
        "cm1 = metrics.confusion_matrix(y_test,rf_predict)\n",
        "disp = metrics.ConfusionMatrixDisplay(\n",
        "    confusion_matrix=cm1, display_labels=clf.classes_)\n",
        "disp.plot()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jZAXgGf0MQe"
      },
      "outputs": [],
      "source": [
        "RCF_preds_comparasion = pd.DataFrame()\n",
        "RCF_preds_comparasion['RCF_prediction'] = y_predict\n",
        "RCF_preds_comparasion['actual_sentiment'] = tweets['Sentiment']\n",
        "RCF_preds_comparasion['tweet'] = tweets['Tweet']\n",
        "RCF_preds_comparasion.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Tq5zNQcGJg1"
      },
      "source": [
        "## Transformer Based Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCy0Nj46oLBP"
      },
      "outputs": [],
      "source": [
        "!pip3 install torch torchvision torchaudio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDJPyhPUIzZg"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "from transformers import pipeline\n",
        "sentiment_pipeline = pipeline(model=\"finiteautomata/bertweet-base-sentiment-analysis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMuLNtud1UI-"
      },
      "outputs": [],
      "source": [
        "def get_sentiment(tweet):\n",
        "    sentiment = sentiment_pipeline(tweet)\n",
        "    label = sentiment[0]['label']\n",
        "   # print(label, tweet)\n",
        "    return label\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdLdTZk3RpSY"
      },
      "outputs": [],
      "source": [
        "def get_score(tweet):\n",
        "    sentiment = sentiment_pipeline(tweet)\n",
        "    score = sentiment[0]['score']\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhQGJjLb1D2p"
      },
      "outputs": [],
      "source": [
        "tweets['Transformer Sentiment'] = tweets['tweet_updated'].apply(lambda x: get_sentiment(x))\n",
        "tweets['Transformer Score'] = tweets['tweet_updated'].apply(lambda x: get_score(x))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets[\"Transformer Sentiment\"] = tweets[\"Transformer Sentiment\"].replace('NEU', 'neutral')\n",
        "tweets[\"Transformer Sentiment\"] = tweets[\"Transformer Sentiment\"].replace('POS', 'positive')\n",
        "tweets[\"Transformer Sentiment\"] = tweets[\"Transformer Sentiment\"].replace('NEG', 'negative')"
      ],
      "metadata": {
        "id": "lgXFkyaVsG8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tb_predict = tweets['Transformer Sentiment']"
      ],
      "metadata": {
        "id": "5W-vjv9nsi6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMIYdEmIg7mD"
      },
      "outputs": [],
      "source": [
        "fix_sentiment(tweets['vader_analysis'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-P5sGise6AR"
      },
      "outputs": [],
      "source": [
        "tweets.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pgzPPbSGJg2"
      },
      "outputs": [],
      "source": [
        "accuracy_score3 = metrics.balanced_accuracy_score(tweets['Sentiment'],tb_predict)\n",
        "print(\"Transformer Base Accuracy = {:.2f}%\".format(accuracy_score3*100))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Classification Report:{classification_report(tweets['Sentiment'],tb_predict)}\")"
      ],
      "metadata": {
        "id": "R677dtSksSw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate confusion matrix\n",
        "cm2 = metrics.confusion_matrix(tweets['Sentiment'],tb_predict)\n",
        "disp = metrics.ConfusionMatrixDisplay(\n",
        "    confusion_matrix=cm2, display_labels=clf.classes_)\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RuJzRhJFuOKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yn9DwcggoLBS"
      },
      "outputs": [],
      "source": [
        "#compare all models\n",
        "preds_comparasion = pd.DataFrame()\n",
        "preds_comparasion['LR_prediction'] = lr_predict\n",
        "preds_comparasion['NB_prediction'] = nb_predict\n",
        "preds_comparasion['RF_prediction'] = rf_predict\n",
        "preds_comparasion['TB_prediction'] = tb_predict\n",
        "\n",
        "preds_comparasion['original_sentiment'] = tweets['Sentiment']\n",
        "preds_comparasion['vader_sentiment'] = tweets['vader_analysis']\n",
        "preds_comparasion['tweet'] = tweets['Tweet']\n",
        "preds_comparasion.head(50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7W3p9y5oLBS"
      },
      "source": [
        "The random forest model seems to has the worst accuracy score as well as the most disagreements with the other models' sentiment taggings. "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "fc40fbc46230ce2b79c331b45297241116f85022f5390bbf3c4a432c404448cb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}